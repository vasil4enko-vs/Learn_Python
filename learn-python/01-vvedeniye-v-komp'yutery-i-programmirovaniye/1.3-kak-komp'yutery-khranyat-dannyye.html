<!-- @format -->

<!DOCTYPE html>
<html lang="ru">
    <head>
        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Хранение данных в компьютере</title>
    </head>
    <body>
        <h1>1.3. Хранение данных в компьютере</h1>
        <p><strong>Ключевые положения: </strong></p>
        <h2>1.3.1. Единицы измерения количества информации</h2>
        <p>
            Все данные хранятся в компьютере в виде последовательностей,
            состоящих из гулей и единиц.
        </p>
        <p>
            Память компьютера разделена на единицы хранения, которые называются
            <em>байтами</em>.
        </p>
        <p>
            <strong>Байт</strong> (англ. <strong><em>byte</em></strong
            >) (русское обозначение: байт и Б; международное: byte и B) —
            единица хранения и обработки цифровой информации; совокупность
            битов, обрабатываемая компьютером одновременно. В современных
            вычислительных системах байт состоит из 8 бит и, соответственно,
            может принимать одно из 256 (от 0 до 255) различных значений
            (состояний, кодов).
        </p>
        <p>
            Как видно из определения каждый байт разделен на восемь меньших
            единиц хранения, которые называются битами, или разрядами.
        </p>
        <p>
            <strong>Бит</strong> (русское обозначение: <strong>бит</strong>;
            международное: <strong>bit</strong>; от англ.
            <strong><em>binary digit</em></strong> — двоичная цифра; также игра
            слов: англ. <strong><em>bit</em></strong> — кусочек, частица) —
            единица измерения количества информации. 1 бит информации — символ
            или сигнал, который может принимать два значения: включено или
            выключено, да или нет, высокий или низкий, заряженный или
            незаряженный; в двоичной системе исчисления это 1 (единица) или 0
            (ноль). Это минимальное количество информации, которое необходимо
            для ликвидации минимальной неопределенности.
        </p>
        <p>
            Обычно программисты рассматривают байт как восемь переключателей,
            каждый из которых может иметь два положения "Включено" и
            "Выключено."
        </p>
        <img
            src="01-images/1.2-predstavleniye-bayta-v-vide-vosmi-pereklyuchateley-(bitov).png"
            alt="Представление байта в виде восьми переключателей (битов)"
            loading="lazy"
            width="800" />
        <p>Рис.1.2. Представление байта в виде восьми переключателей (битов)</p>
        <h2>1.3.2. Хранение чисел</h2>
        <p>
            Итак, в компьютерных системах выключенный бит представляет число 0,
            а включенный - число 1. Что делает его использование для
            представления чисел весьма ограниченным. Но это же делает его
            идеальным для применения с <em>двоичной системы исчисления</em>, в
            которой все числовые значения записываются как последовательности
            нулей и единиц.
        </p>
        <p>Пример числа записанного в двоичной форме:</p>
        <pre>
            <code>
                10011101
            </code>
        </pre>
        <p>
            Позиция каждой цифры в двоичном числе соответствует определённому
            значению. Как показано на рис. 1.3 слева, начиная с самой правой
            цифры и двигаясь влево значения позиций равняются:
            <em
                >2<sup>0</sup>, 2<sup>1</sup>, 2<sup>2</sup> ..., 2<sup
                    >7</sup
                ></em
            >.
        </p>
        <p>
            На этом же рисунке справа, показано та же схема, но значения уже
            вычислены: <em>1, 2, 4, ..., 128</em>.
        </p>
        <img
            src="01-images/1.3-znacheniya-dvoichnykh_razryadov.png"
            alt="Значения двоичных разрядов"
            loading="lazy"
            width="800" />
        <p>Рис.1.3. Значения двоичных разрядов</p>
        <p>
            Для того чтобы определить значение двоичного числа, нужно сложить
            позиционные значения всех единиц. так, в двоичном числе 10011101
            позиционные значения единиц равняются 1, 4, 8, 16, 128. Сложив эти
            значения получим 157 (рис.1.4 слева).
        </p>
        <p>
            Значит, двоичное число 10011101 равняется 157 в десятичной системе
            исчисления.
        </p>
        <img
            src="01-images/1.4-perevod-iz-dvoichnogo-ischesleniya-v-desyatichnoye.png"
            alt="Перевод из двоичного исчисления в десятичное"
            loading="lazy"
            width="800" />
        <p>Рис.1.4. Перевод из двоичного исчисления в десятичное</p>
        <p>
            На рис. 1.4 справа показано, как можно изобразить хранение числа 157
            в байте оперативной памяти. Каждая единица представлена битом в
            положении "Вкл.", а каждый ноль - битом в положении "Выкл."
        </p>
        <p>
            Когда всем битам в байте назначены нули, т.е. они выключены,
            значение байта равняется 0. Когда всем битам в байте назначены
            единицы (они включены), байт содержит самое большое значение,
            которое в нём может быть размещено. Оно равняется 1 + 2 + 4 + 8 + 16
            + 32 + 64 + 128 = 255. Этот предел является следствием того, что в
            байте всего 8 бит.
        </p>
        <p>
            Если необходимо записать число больше 255, нужно использовать ещё
            один байт. При использовании двух байтов вместе в итоге получается
            16 бит. Позиционные этих 16 бит будут 2<sup>0</sup> 2<sup>1</sup>,
            2<sup>2</sup> ... 2<sup>15</sup>. Максимальное значение, которое
            можно разместить в двух байтах рано
            <strong>65 535</strong> (рис.1.5). Если же нужно будет записать ещё
            большее число, то потребуется больше байтов.
        </p>
        <img
            src="01-images/1.5-ispolzovaniye dvukh baytov dlya predstavleniya chisla bolshe 255.png"
            alt="Перевод из двоичного исчисления в десятичное"
            loading="lazy"
            width="800" />
        <p>
            Рис. 1.5. Использование двух байтов для представления числа больше
            255
        </p>
        <h2>1.3.2. Хранение символов</h2>
        <p>
            Любые данные в компьютере представлены в виде двоичных чисел. В том
            числе и символы, такие как буквы и знаки препинания.
        </p>
        <p>
            За прошедшие годы для представления символов в памяти компьютера
            были разработаны различные стандарты кодирования. Исторически очень
            важным является стандарт кодирования <strong>ASCII</strong> (<em
                ><strong
                    >American Standard Code for Information Interchange</strong
                ></em
            >). Аббревиатура ASCII произносится "аски".
        </p>
        <p>
            Однако у стандарта кодирования ASCII есть существенный недостаток,
            он определяет коды только для 128 символов.
        </p>
        <p>
            Для исправления этого недостатка в 1992 году Кеном Томпсоном и Робом
            Пайком был разработан стандарт кодирования <strong>UTF-8</strong>.
        </p>
        <p>
            Кодировка UTF-8 в настоящее время является доминирующей в
            веб-пространстве, нашла широкое применение в UNIX-подобных
            операционных системах и фактически стала стандартным набором
            символов, используемых в компьютерной индустрии.
        </p>
        <p>Что же такое "стандарт кодирования символов UTF-8"?</p>
        <p>
            Википедия даёт такое
            <a href="https://ru.wikipedia.org/wiki/UTF-8">определение</a>:
        </p>
        <p>
            <strong>UTF-8</strong> (от англ.
            <em><strong>Unicode Transformation Format, 8-bit</strong></em> -
            формат преобразования Юникода, 8-бит) - стандарт кодирования
            символов, позволяющий компактно хранить и передавать символы
            Юникода, используя переменное количество байт (от одного до четырёх)
            и обеспечивающий полную обратную совместимость с 7-битной кодировкой
            ASCII. Стандарт UTF-8 официально закреплён в документах
            <strong>RFC 3629</strong> и <strong>ISO/IEC 10646 Annex D</strong>.
        </p>
        <p>
            Примером может служить кодирование прописной английской (латинской)
            буквы <em>A</em>. Когда на компьютерной клавиатуре набирают букву
            <em>A</em> в верхнем регистре, в памяти ПК сохраняется число
            <em>65</em>, в двоичном коде - <em>01000001</em>(рис. 1.6).
        </p>
        <img
            src="01-images/1.6-kodirovaniye-angliyskoy-bukvy-a.png"
            alt="Кодирование английской буквы A"
            loading="lazy"
            width="800" />
        <p>
            Для других символов также есть свои коды, например, прописные
            английские:
        </p>
        <ul>
            <li><em>B</em> - 66 (01000010):</li>
            <li><em>C</em> - 67 (01000011),</li>
        </ul>
        <p>а строчная английская <em>a</em> кодируется как - 97 (01100001).</p>
        <h2>1.3.3. Хранение чисел повышенной сложности</h2>
        <p>
            Ознакомившись с хранением чисел в памяти можно подумать, что
            двоичная система счисления может использоваться только для целых
            положительных чисел и нуля. Представить отрицательные и вещественные
            числа (такие как 3,14) при помощи рассмотренного выше метода
            невозможно.
        </p>
        <p>
            Однако компьютеры способны хранить и отрицательные, и вещественные
            числа. Для этого в них используются специальные схемы кодирования.
            Отрицательные числа кодируются с помощью метода
            <a href="https://wiki5.ru/wiki/Two's_complement"
                >дополнения до двух</a
            >, а вещественные числа - в
            <a href="https://ru.wikipedia.org/wiki/Число_с_плавающей_запятой"
                >форме записи с плавающей точкой (запятой)</a
            >.
        </p>
        <p>
            Не важно понимать как эти схемы работают, главное знать, что они
            используются для представления отрицательных и вещественных чисел в
            двоичном формате.
        </p>
        <h2>1.3.4. Другие типы данных</h2>
    </body>
</html>
